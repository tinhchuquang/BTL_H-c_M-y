{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Julie', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Linda', 1)])\n",
      "dict_items([('Jane', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Julie', 1), ('loves', 1)])\n",
      "dict_items([('He', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('baseball', 1)])\n"
     ]
    }
   ],
   "source": [
    "mydoclist = ['Julie loves me more than Linda loves me',\n",
    "'Jane likes me more than Julie loves me',\n",
    "'He likes basketball more than baseball']\n",
    "\n",
    "#mydoclist = ['sun sky bright', 'sun sun bright']\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] +=1\n",
    "    print(tf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [basketball, me, He, more, Julie, than, Linda, likes, baseball, Jane, loves]\n",
      "The doc is \"Julie loves me more than Linda loves me\"\n",
      "The tf vector for Document 1 is [0, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2]\n",
      "The doc is \"Jane likes me more than Julie loves me\"\n",
      "The tf vector for Document 2 is [0, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "The doc is \"He likes basketball more than baseball\"\n",
      "The tf vector for Document 3 is [1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
      "All combined, here is our master document term matrix: \n",
      "[[0, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2], [0, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import string #allows for format()\n",
    "    \n",
    "def build_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def tf(term, document):\n",
    "  return freq(term, document)\n",
    "\n",
    "def freq(term, document):\n",
    "  return document.split().count(term)\n",
    "\n",
    "vocabulary = build_lexicon(mydoclist)\n",
    "\n",
    "doc_term_matrix = []\n",
    "print('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "for doc in mydoclist:\n",
    "    print('The doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    print('The tf vector for Document %d is [%s]' % ((mydoclist.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    \n",
    "    # here's a test: why did I wrap mydoclist.index(doc)+1 in parens?  it returns an int...\n",
    "    # try it!  type(mydoclist.index(doc) + 1)\n",
    "\n",
    "print('All combined, here is our master document term matrix: ')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old document term matrix: \n",
      "[[0 2 0 1 1 1 1 0 0 0 2]\n",
      " [0 2 0 1 1 1 0 1 0 1 1]\n",
      " [1 0 1 1 0 1 0 1 1 0 0]]\n",
      "\n",
      "A document term matrix with row-wise L2 norms of 1:\n",
      "[[ 0.          0.57735027  0.          0.28867513  0.28867513  0.28867513\n",
      "   0.28867513  0.          0.          0.          0.57735027]\n",
      " [ 0.          0.63245553  0.          0.31622777  0.31622777  0.31622777\n",
      "   0.          0.31622777  0.          0.31622777  0.31622777]\n",
      " [ 0.40824829  0.          0.40824829  0.40824829  0.          0.40824829\n",
      "   0.          0.40824829  0.40824829  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "\n",
    "print('A regular old document term matrix: ') \n",
    "print(np.matrix(doc_term_matrix))\n",
    "print('\\nA document term matrix with row-wise L2 norms of 1:')\n",
    "print(np.matrix(doc_term_matrix_l2))\n",
    "\n",
    "# if you want to check this math, perform the following:\n",
    "# from numpy import linalg as la\n",
    "# la.norm(doc_term_matrix[0])\n",
    "# la.norm(doc_term_matrix_l2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ecd4e40c7ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmy_idf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydoclist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Our vocabulary vector is ['\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m']'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount +=1\n",
    "    return doccount \n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / 1+df)\n",
    "\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "\n",
    "print('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A =np.matrix('0, 0, 0; 1, 0, 1; 1, 0, 1')\n",
    "W = np.matrix('1,1,1;1,0,0;-1,-1,0')\n",
    "C = np.sum(A*W)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A =np.matrix('0, 0, 0; 2, 0, 2; 0, 0, 2')\n",
    "W = np.matrix('0,0,0;-1,-1,1;1,1,1')\n",
    "C = np.sum(A.dot(W))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A =np.matrix('0, 0, 0; 1, 2, 1; 0, 0, 2')\n",
    "W = np.matrix('0,0,-1;0,1,-1;0,1,0')\n",
    "C = np.sum(A.dot(W))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
